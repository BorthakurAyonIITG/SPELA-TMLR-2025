{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './../../../Models')\n",
    "from sphere_points import generate_points\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import normalize, one_hot\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "import torchvision\n",
    "from torchvision.transforms import *\n",
    "# from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data/train_32x32.mat\n",
      "Using downloaded and verified file: ./data/test_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# load the dataset\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), \n",
    "    transforms.Resize((64, 64))]) # this normalizes to [0,1]\n",
    "trainset = torchvision.datasets.SVHN(root='./data', split=\"train\", \n",
    "                                     download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=64,\n",
    "                                          shuffle=True, drop_last=True, \n",
    "                                           collate_fn = lambda x: tuple(x_.to(device) for x_ in default_collate(x)))\n",
    "testset = torchvision.datasets.SVHN(root='./data', split=\"test\", \n",
    "                                    download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=64,\n",
    "                                         shuffle=False, \n",
    "                                          collate_fn = lambda x: tuple(x_.to(device) for x_ in default_collate(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_loss = nn.CrossEntropyLoss()\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_head_train(inp_embedding, classifier_weights, labels):\n",
    "    inp_embedding = normalize(inp_embedding, p=2, dim=-1)\n",
    "    classifier_output = torch.mm(inp_embedding, classifier_weights)\n",
    "    classifier_output = classifier_output * one_hot(labels, num_classes = num_classes).type(torch.float32)\n",
    "    theta = 1\n",
    "    loss = torch.mean(torch.log(2 - (theta * torch.sum(classifier_output,1))))\n",
    "    # classifier_output = torch.softmax(classifier_output, dim=-1)\n",
    "    # loss = model_loss(classifier_output, one_hot(labels, num_classes = num_classes).type(torch.float32))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_head_test(inp_embedding, classifier_weights, labels):\n",
    "    inp_embedding = normalize(inp_embedding, p=2, dim=-1)\n",
    "    classifier_output = torch.mm(inp_embedding, classifier_weights)\n",
    "    # classifier_output = 1 - (torch.acos(classifier_output)/np.pi)\n",
    "    # classifier_output = torch.softmax(classifier_output, dim=-1)\n",
    "    # loss = model_loss(classifier_output, one_hot(labels, num_classes = num_classes).type(torch.float32))\n",
    "    return torch.argmax(classifier_output, dim=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "# theta = 1\n",
    "# cos_similarity = nn.CosineSimilarity(dim = 1, eps = 1e-6)\n",
    "mse_loss = nn.MSELoss()\n",
    "initial = None\n",
    "\n",
    "# def loss_layer(y_pos, y_neg):\n",
    "#     return torch.mean(torch.log(2 - (theta * cos_similarity(y_pos, y_neg))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (num_data, num_features) => no dimension for batch size please\n",
    "class Layer(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, dropout_prob, bias, device, lr):\n",
    "        super().__init__(in_features, out_features, bias, device)\n",
    "        self.out_features = out_features\n",
    "        self.bias_flag = bias\n",
    "        self.lr = lr\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.dropout = nn.Dropout(p = self.dropout_prob)\n",
    "        self.num_classes = 10\n",
    "        self.dimension = out_features\n",
    "        self.leaky_relu = nn.PReLU(init = 0.001)\n",
    "        self.opt = Adam(self.parameters(), lr = self.lr)\n",
    "        global initial\n",
    "        nn.init.kaiming_normal_(self.weight, mode='fan_in')\n",
    "        # fc1_limit = np.sqrt(6.0 / in_features)\n",
    "        # torch.nn.init.uniform_(self.weight, a=-fc1_limit, b=fc1_limit)\n",
    "        self.directions = generate_points(self.num_classes, self.dimension, steps = 10000, initial_points = initial)\n",
    "        initial = np.array(self.directions)\n",
    "        self.directions = [torch.tensor(t, dtype = torch.float32).to(device) for t in self.directions]\n",
    "        self.direction_weights = torch.zeros((len(self.directions[0]), len(self.directions)), device=device)\n",
    "        for i in range(len(self.directions)):\n",
    "            self.direction_weights[:, i] = normalize(self.directions[i], p = 2, dim=-1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_direction = normalize(x, p = 2, dim = 1)\n",
    "        if self.bias_flag:\n",
    "            return self.leaky_relu(self.dropout(torch.mm(x_direction, self.weight.T) + self.bias.unsqueeze(0)))\n",
    "        else:\n",
    "            return self.leaky_relu(self.dropout(torch.mm(x_direction, self.weight.T)))\n",
    "\n",
    "    def train(self, x, labels):\n",
    "        # opt = Adam(self.parameters(), lr = self.lr)\n",
    "        y = self.forward(x) # shape: (num_data, out_features)\n",
    "        y = normalize(y, p = 2, dim = 1)\n",
    "        '''\n",
    "        directions = torch.zeros_like(y)\n",
    "        for i in range(y.shape[0]):\n",
    "            directions[i, :] = self.directions[label[i]].reshape(1, -1)\n",
    "        loss = loss_layer(y, directions)\n",
    "        '''\n",
    "        loss = classifier_head_train(y, self.direction_weights, labels)\n",
    "        self.opt.zero_grad()\n",
    "        loss.backward(retain_graph = True)\n",
    "        self.opt.step()\n",
    "\n",
    "        return loss.item(), y\n",
    "    \n",
    "    def test(self, x, labels):\n",
    "        with torch.no_grad():\n",
    "            y = self.forward(x)\n",
    "            # max_idx_list \n",
    "            '''\n",
    "            for dat in range(y.shape[0]):\n",
    "                max = -np.inf\n",
    "                max_idx = 0\n",
    "                for i in range(self.num_classes):\n",
    "                    cos_sim = cos_similarity(y[dat, :].unsqueeze(0), self.directions[i].reshape(1, -1))\n",
    "                    if cos_sim > max:\n",
    "                        max = cos_sim\n",
    "                        max_idx = i\n",
    "                max_idx_list.append(max_idx)\n",
    "            '''\n",
    "            max_idx_list = []\n",
    "            max_idx_list = classifier_head_test(y, self.direction_weights, labels)\n",
    "        return torch.tensor(max_idx_list), y\n",
    "\n",
    "class Layer_Net(nn.Module):\n",
    "    def __init__(self, dims_list, dropout_list, bias, epochs, lr, device):\n",
    "        super(Layer_Net, self).__init__()\n",
    "        self.dims_list = dims_list\n",
    "        self.dropout_list = dropout_list\n",
    "        self.bias = bias\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.device = device\n",
    "        self.layers = []\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        global initial\n",
    "        for d in range(len(self.dims_list) - 1):\n",
    "            self.layers += [Layer(self.dims_list[d], self.dims_list[d + 1], self.dropout_list[d], \n",
    "                                  self.bias, self.device, self.lr).to(self.device)]\n",
    "        \n",
    "    def train(self, data_loader):\n",
    "        layer_loss_list = []\n",
    "        for i in range(len(self.layers)):\n",
    "            layer_loss_list.append([])\n",
    "        pbar = tqdm(total = self.epochs * len(data_loader) * len(self.layers), desc = f\"Training\", position = 0, leave = True)\n",
    "        for epoch in range(self.epochs):\n",
    "            loss_agg = [0] * len(self.layers)\n",
    "            for dat in data_loader:\n",
    "                x = dat[0]\n",
    "                label = dat[1]\n",
    "                for i in range(len(self.layers)):\n",
    "                    # with torch.no_grad():\n",
    "                    #     y = self.layers[i].forward(x)\n",
    "                    loss, y = self.layers[i].train(x, label)\n",
    "                    x = y.detach()\n",
    "                    loss_agg[i] += loss / len(data_loader)\n",
    "                    # self.layers[i].zero_grad(set_to_none=True)\n",
    "                    pbar.update(1)\n",
    "            pbar.set_postfix(epoch = epoch + 1, loss = loss_agg)\n",
    "            for i in range(len(self.layers)):\n",
    "                layer_loss_list[i].append(loss_agg[i])\n",
    "        pbar.close()\n",
    "        return layer_loss_list\n",
    "\n",
    "        \n",
    "    def test(self, data_loader):\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            cm_preds = []\n",
    "            cm_labels = []\n",
    "            for dat in tqdm(data_loader, desc = \"Testing\"):\n",
    "                x = dat[0]\n",
    "                label = dat[1]\n",
    "                preds = []\n",
    "                for i in range(len(self.layers)):\n",
    "                    pred, x = self.layers[i].test(x, label)\n",
    "                    preds.append(pred)\n",
    "                correct += (preds[-1] == label.cpu()).sum().item()\n",
    "                cm_preds += preds[-1].cpu().numpy().tolist()\n",
    "                cm_labels += label.cpu().numpy().tolist()\n",
    "                total += label.shape[0]\n",
    "        return correct / total, [cm_preds, cm_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_kernel_classes = None\n",
    "\n",
    "# Takes input, size of kernel and gives output dimension of cuboid\n",
    "def gen_size_lists(h_init_image, w_init_image, kernel_size_list, pooling_kernels, padding_list = None, stride_list = None):\n",
    "    h_list = [h_init_image]\n",
    "    w_list = [w_init_image]\n",
    "\n",
    "    new_kernel_size_list = []\n",
    "    k = len(kernel_size_list) + len(pooling_kernels)\n",
    "    for i in range(k):\n",
    "        if i % 2 == 0:\n",
    "            new_kernel_size_list.append(kernel_size_list[i // 2])\n",
    "        else:\n",
    "            new_kernel_size_list.append(pooling_kernels[i // 2])\n",
    "    \n",
    "    if stride_list is None:\n",
    "        stride_list = [1] * len(new_kernel_size_list)\n",
    "    if padding_list is None:\n",
    "        padding_list = [0] * len(new_kernel_size_list)\n",
    "    \n",
    "    for i in range(k):\n",
    "        if i % 2 == 1:\n",
    "            if new_kernel_size_list[i] is not None:\n",
    "                stride_list[i] = new_kernel_size_list[i][0]\n",
    "\n",
    "    for i in range(len(new_kernel_size_list)):\n",
    "        if new_kernel_size_list[i] is None:\n",
    "            h_list.append(int(h_list[i]))\n",
    "            w_list.append(int(w_list[i]))\n",
    "        else:\n",
    "            h_list.append(int((h_list[i] + 2 * padding_list[i] - new_kernel_size_list[i][0]) / stride_list[i] + 1))\n",
    "            w_list.append(int((w_list[i] + 2 * padding_list[i] - new_kernel_size_list[i][1]) / stride_list[i] + 1))\n",
    "    \n",
    "    h_list = h_list[1 : ]\n",
    "    w_list = w_list[1 : ]\n",
    "\n",
    "    final_h_list, final_w_list = [], []\n",
    "\n",
    "    for i in range(len(h_list) // 2):\n",
    "        if new_kernel_size_list[2 * i + 1] is not None:\n",
    "            final_h_list.append(h_list[2 * i + 1])\n",
    "            final_w_list.append(w_list[2 * i + 1])\n",
    "        else:\n",
    "            final_h_list.append(h_list[2 * i])\n",
    "            final_w_list.append(w_list[2 * i])\n",
    "            \n",
    "    return final_h_list, final_w_list, new_kernel_size_list\n",
    "\n",
    "class Conv_Layer(nn.Conv2d, nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels, \n",
    "                 out_channels, \n",
    "                 kernel_size,\n",
    "                 batch_norm_flag,  \n",
    "                 pooling_size, \n",
    "                 pooling_type, \n",
    "                 bias, \n",
    "                 device, \n",
    "                 lr, \n",
    "                 h_out, \n",
    "                 w_out, \n",
    "                 symm_vector_dim, \n",
    "                 n_symm_vectors):\n",
    "        super(Conv_Layer, self).__init__(in_channels, out_channels, kernel_size)\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.batch_norm_flag = batch_norm_flag\n",
    "        if self.batch_norm_flag:\n",
    "            self.batch_norm = nn.BatchNorm2d(self.out_channels)\n",
    "        else:\n",
    "            self.batch_norm = nn.Identity()\n",
    "        \n",
    "        self.pooling_size = pooling_size\n",
    "        self.pooling_type = pooling_type\n",
    "        self.bias_flag = bias\n",
    "        self.device = device\n",
    "        self.lr = lr\n",
    "        self.h_out = h_out\n",
    "        self.w_out = w_out\n",
    "        self.D = self.h_out * self.w_out\n",
    "        self.symm_vector_dim = symm_vector_dim\n",
    "        self.n_symm_vectors = n_symm_vectors\n",
    "        # Initialize the projection matrix such that each kernel has a trainable projection matrix \n",
    "        # size of input is (batch_size x num_kernels x (h_out * w_out) \n",
    "        # output is (batch_size x num_kernels x symm_vector_dim)\n",
    "        self.mlp_weights = nn.Parameter(torch.normal(0, 1, size = (self.out_channels, self.D, self.symm_vector_dim)).to(self.device), requires_grad = True)\n",
    "\n",
    "        self.cos_similarity_2 = nn.CosineSimilarity(dim = 2, eps = 1e-6)\n",
    "        self.cos_similarity_3 = nn.CosineSimilarity(dim = 3, eps = 1e-6)\n",
    "\n",
    "        global num_classes\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        global init_kernel_classes\n",
    "        if init_kernel_classes is None:\n",
    "            # Assign the classes to each kernel\n",
    "            self.kernel_classes = []\n",
    "            for i in range(self.out_channels):\n",
    "                group_number = list(range(self.n_symm_vectors[i]))\n",
    "                flag = True\n",
    "                while flag:\n",
    "                    group_list = np.random.choice(group_number, self.num_classes, replace = True).tolist()\n",
    "                    if len(set(group_list)) == self.n_symm_vectors[i]:\n",
    "                        flag = False\n",
    "                self.kernel_classes.append(group_list)\n",
    "            init_kernel_classes = self.kernel_classes\n",
    "        else:\n",
    "            self.kernel_classes = init_kernel_classes\n",
    "\n",
    "        self.leaky_relu = nn.LeakyReLU(negative_slope = 0.0001)\n",
    "        \n",
    "        self.pool = None\n",
    "        if self.pooling_size is not None and self.pooling_type is not None:\n",
    "            if self.pooling_type == \"max\":\n",
    "                self.pool = nn.MaxPool2d(self.pooling_size[0])\n",
    "            elif self.pooling_type == \"avg\":\n",
    "                self.pool = nn.AvgPool2d(self.pooling_size[0])\n",
    "\n",
    "        # self.fac = nn.Parameter(torch.tensor(10.0, dtype = torch.float32).to(self.device), requires_grad = True)\n",
    "        \n",
    "        self.opt = Adam(self.parameters(), lr = self.lr)\n",
    "\n",
    "        self.tmp_directions = []\n",
    "        for i in range(self.out_channels):\n",
    "            self.tmp_directions.append(generate_points(self.n_symm_vectors[i], self.symm_vector_dim, steps = 10000, initial_points = None))\n",
    "        self.tmp_directions = [torch.tensor(np.stack(t), dtype = torch.float32).to(self.device) for t in self.tmp_directions]\n",
    "        \n",
    "        # Shape of kernel_directions: (out_channels, n_symm_vectors, symm_vector_dim)\n",
    "        # Broadcast to shape of: (out_channels, num_classes, symm_vector_dim)\n",
    "        \n",
    "        self.kernel_directions = torch.zeros((self.out_channels, self.num_classes, self.symm_vector_dim))\n",
    "        for i in range(self.out_channels):\n",
    "            for j in range(self.num_classes):\n",
    "                self.kernel_directions[i, j, :] = self.tmp_directions[i][self.kernel_classes[i][j], :]\n",
    "        \n",
    "        self.test_kernel_directions = self.kernel_directions.unsqueeze(0).repeat(batch_size, 1, 1, 1).to(self.device)     \n",
    "        self.kernel_directions = torch.swapaxes(self.kernel_directions, 0, 1).to(self.device)\n",
    "        self.all_ones = torch.ones([batch_size, self.out_channels]).to(self.device)\n",
    "\n",
    "    def _conv_forward(self, input, weight, bias):\n",
    "        return F.conv2d(input, weight, bias, self.stride,\n",
    "                        self.padding, self.dilation, self.groups)\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = self._conv_forward(input, self.weight, self.bias)\n",
    "        out = self.batch_norm(out)\n",
    "        out = self.leaky_relu(out)\n",
    "        if self.pooling_size is not None:\n",
    "            out = self.pool(out)\n",
    "        return out\n",
    "    \n",
    "    def train(self, x, labels):\n",
    "        # opt = Adam(self.parameters(), lr = self.lr)\n",
    "        # Shape of y: (num_data, out_channels, h_out, w_out)\n",
    "        y = self.forward(x)\n",
    "        # New Shape of y: (num_data, out_channels, h_out * w_out)\n",
    "        z = torch.flatten(y, start_dim = 2)\n",
    "        proj_y = torch.matmul(z.unsqueeze(2), self.mlp_weights).squeeze(2)\n",
    "        proj_y = normalize(proj_y, p = 2, dim = 2)\n",
    "        cos_sim = self.cos_similarity_2(proj_y, self.kernel_directions[labels, :, :])\n",
    "        loss = mse_loss(self.all_ones[:cos_sim.shape[0], :], cos_sim)\n",
    "        self.opt.zero_grad()\n",
    "        loss.backward(retain_graph = True)\n",
    "        self.opt.step()\n",
    "\n",
    "        return loss.item(), y\n",
    "\n",
    "    def test(self, x):\n",
    "        with torch.no_grad():\n",
    "            y = self.forward(x)\n",
    "            z = torch.flatten(y, start_dim = 2)\n",
    "            proj_y = torch.matmul(z.unsqueeze(2), self.mlp_weights).squeeze(2)\n",
    "            proj_y = normalize(proj_y, p = 2, dim = 2)\n",
    "            proj_y = proj_y.unsqueeze(2).repeat(1, 1, self.num_classes, 1)\n",
    "            cos_sim = self.cos_similarity_3(proj_y, self.test_kernel_directions[ : proj_y.shape[0], :, :, :])\n",
    "            cos_sim = torch.mean(cos_sim, dim = 1).cpu()\n",
    "        return torch.argmax(cos_sim, dim = 1), y\n",
    "\n",
    "# dims list will be of the form [(in_features_1, out_features_1), (in_features_2, out_features_2), ...]\n",
    "class Conv_Net(nn.Module):\n",
    "    def __init__(self, \n",
    "                 conv_list, \n",
    "                 kernel_size_list, \n",
    "                 batch_norm_list, \n",
    "                 pooling_size_list, \n",
    "                 pooling_type, \n",
    "                 epochs, bias, \n",
    "                 device, lr, \n",
    "                 h_list, \n",
    "                 w_list, \n",
    "                 symm_vector_dim_list, \n",
    "                 n_symm_vectors, \n",
    "                 group_layers):\n",
    "        super().__init__()\n",
    "        self.conv_list = conv_list\n",
    "        self.kernel_size_list = kernel_size_list\n",
    "        self.batch_norm_list = batch_norm_list\n",
    "        self.pooling_size_list = pooling_size_list\n",
    "        self.pooling_type = pooling_type\n",
    "        self.epochs = epochs\n",
    "        self.bias_flag = bias\n",
    "        self.lr = lr\n",
    "        self.device = device\n",
    "        self.h_list = h_list\n",
    "        self.w_list = w_list\n",
    "        self.symm_vector_dim_list = symm_vector_dim_list\n",
    "        self.n_symm_vectors = n_symm_vectors\n",
    "        self.group_layers = group_layers\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        global init_kernel_classes\n",
    "        \n",
    "        for i in range(len(self.kernel_size_list)):\n",
    "            if i == 0:\n",
    "                init_kernel_classes = None\n",
    "            elif self.group_layers[i] != self.group_layers[i - 1]:\n",
    "                init_kernel_classes = None\n",
    "\n",
    "            self.layers.append(Conv_Layer(self.conv_list[i][0], \n",
    "                                          self.conv_list[i][1], \n",
    "                                          self.kernel_size_list[i],\n",
    "                                          self.batch_norm_list[i], \n",
    "                                          self.pooling_size_list[i],\n",
    "                                          self.pooling_type[i],\n",
    "                                          self.bias_flag, \n",
    "                                          self.device, \n",
    "                                          self.lr, \n",
    "                                          self.h_list[i], \n",
    "                                          self.w_list[i], \n",
    "                                          self.symm_vector_dim_list[i],\n",
    "                                          self.n_symm_vectors[i]))\n",
    "        \n",
    "        print(self.layers)\n",
    "    \n",
    "    def train(self, data_loader):\n",
    "       \n",
    "        layer_loss_list = []\n",
    "        for i in range(len(self.layers)):\n",
    "            layer_loss_list.append([])\n",
    "        pbar = tqdm(total = self.epochs * len(data_loader) * len(self.layers), desc = f\"Training\", position = 0, leave = True)\n",
    "        for epoch in range(self.epochs):\n",
    "            loss_agg = [0] * len(self.layers)\n",
    "            for dat in data_loader:\n",
    "                x, label = dat\n",
    "                for i in range(len(self.layers)):\n",
    "                    # with torch.no_grad():\n",
    "                    #     y = self.layers[i].forward(x)\n",
    "                    loss, y = self.layers[i].train(x, label)\n",
    "                    # self.layers[i].zero_grad(set_to_none=True)\n",
    "                    x = y.detach()\n",
    "                    loss_agg[i] += loss / len(data_loader)\n",
    "                    pbar.update(1)\n",
    "            pbar.set_postfix(epoch = epoch + 1, loss = loss_agg)\n",
    "            for i in range(len(self.layers)):\n",
    "                layer_loss_list[i].append(loss_agg[i])\n",
    "        pbar.close()\n",
    "        return layer_loss_list\n",
    "    \n",
    "    def forward_pass(self, x):\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(self.layers)):\n",
    "                x = self.layers[i].forward(x)\n",
    "        return x\n",
    "\n",
    "    def test(self, data_loader):\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            cm_preds = []\n",
    "            cm_labels = []\n",
    "            acc_list = [0] * len(self.layers)\n",
    "            for dat in tqdm(data_loader, desc = \"Testing\"):\n",
    "                x, label = dat\n",
    "                preds = []\n",
    "                for i in range(len(self.layers)):\n",
    "                    pred, x = self.layers[i].test(x)\n",
    "                    preds.append(pred)\n",
    "                \n",
    "                for i, lst in enumerate(preds):\n",
    "                    acc_list[i] += (lst == label.cpu()).sum().item()\n",
    "                \n",
    "                correct += (preds[-1] == label.cpu()).sum().item()\n",
    "                cm_preds += preds[-1].cpu().numpy().tolist()\n",
    "                cm_labels += label.cpu().numpy().tolist()\n",
    "                total += label.shape[0]\n",
    "            acc_list = [acc / total for acc in acc_list]\n",
    "        return correct / total, [cm_preds, cm_labels], acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_dataloader(data_loader, conv_layer, device):\n",
    "    new_data, new_label = [], []\n",
    "    for (data, label) in tqdm(data_loader, desc = \"New Dataloader\"):\n",
    "        data = conv_layer.forward_pass(data).cpu()\n",
    "        batch_size = data.shape[0]\n",
    "        new_data.append(torch.flatten(data, start_dim = 1))\n",
    "        new_label.append(label.cpu())\n",
    "    \n",
    "    data = torch.cat(new_data, dim = 0)\n",
    "    label = torch.cat(new_label, dim = 0)\n",
    "    new_data_loader = DataLoader(list(zip(data, label)), \n",
    "                                 batch_size = batch_size, shuffle = True,\n",
    "                                 collate_fn = lambda x: tuple(x_.to(device) for x_ in default_collate(x)))\n",
    "\n",
    "    return new_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Full_Net(nn.Module):\n",
    "    def __init__(self, \n",
    "                 conv_list, \n",
    "                 dims_list, \n",
    "                 dropout_list, \n",
    "                 kernel_size_list, \n",
    "                 batch_norm_list, \n",
    "                 pooling_size_list, \n",
    "                 pooling_type, \n",
    "                 epoch_list, \n",
    "                 bias, \n",
    "                 device, \n",
    "                 learning_rate_list, \n",
    "                 h_list, \n",
    "                 w_list, \n",
    "                 symm_vector_dim_list, \n",
    "                 n_symm_vectors,\n",
    "                 group_layers\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.network = Conv_Net(conv_list, \n",
    "                                kernel_size_list,\n",
    "                                batch_norm_list,  \n",
    "                                pooling_size_list, \n",
    "                                pooling_type, \n",
    "                                epoch_list[0], \n",
    "                                bias, \n",
    "                                device, \n",
    "                                learning_rate_list[0], \n",
    "                                h_list, \n",
    "                                w_list, \n",
    "                                symm_vector_dim_list, \n",
    "                                n_symm_vectors,\n",
    "                                group_layers).to(device)\n",
    "        self.layers = Layer_Net(dims_list, \n",
    "                                dropout_list, \n",
    "                                bias, \n",
    "                                epoch_list[1], \n",
    "                                learning_rate_list[1], \n",
    "                                device).to(device)\n",
    "        \n",
    "        self.train_dataloader = None\n",
    "        self.test_dataloader = None\n",
    "        self.device = device\n",
    "        self.cm_train_preds = []\n",
    "        self.cm_test_preds = []\n",
    "\n",
    "    def train(self, dataloader):\n",
    "        layer_loss_list = []\n",
    "        layer_loss_list = self.network.train(dataloader)\n",
    "        # Testing on CNNs\n",
    "        print(f\"\\nInference from CNNs\")\n",
    "        acc, tmp, layerwise_acc_train = self.network.test(dataloader)\n",
    "        print(f\"Train Accuracy: {acc * 100}%\\n\")\n",
    "        self.cm_train_preds.append(tmp)\n",
    "        print(\"Layerwise Train Accuracy:\")\n",
    "        for i, acc in enumerate(layerwise_acc_train):\n",
    "            print(f\"Layer {i + 1}: {acc * 100}%\")\n",
    "        \n",
    "        # have to create a new dataloader with the output of the network\n",
    "        self.train_dataloader = new_dataloader(dataloader, self.network, self.device)\n",
    "        layer_loss_list += self.layers.train(self.train_dataloader)\n",
    "\n",
    "        print(f\"\\nInference from End-to-End Network\")\n",
    "        acc, tmp = self.layers.test(self.train_dataloader)\n",
    "        self.cm_train_preds.append(tmp)\n",
    "        print(f\"Train Accuracy: {acc * 100}%\")\n",
    "        return layer_loss_list\n",
    "    \n",
    "    def test(self, dataloader, flag = False):\n",
    "        print(\"Testing on CNNs\")\n",
    "        cnn_test_acc, tmp, layerwise_acc_test = self.network.test(dataloader)\n",
    "        print(f\"Test Accuracy: {cnn_test_acc * 100}%\\n\")\n",
    "        self.cm_test_preds.append(tmp)\n",
    "        print(\"Layerwise Test Accuracy:\")\n",
    "        for i, acc in enumerate(layerwise_acc_test):\n",
    "            print(f\"Layer {i + 1}: {acc * 100}%\")\n",
    "            \n",
    "        modified_dataloader = new_dataloader(dataloader, self.network, self.device)\n",
    "        if flag:\n",
    "            self.test_dataloader = modified_dataloader\n",
    "        \n",
    "        print(\"Testing on End-to-End Network\")\n",
    "        mlp_test_acc, tmp = self.layers.test(modified_dataloader)\n",
    "        print(f\"Test Accuracy: {mlp_test_acc * 100}%\")\n",
    "        self.cm_test_preds.append(tmp)\n",
    "        \n",
    "        return layerwise_acc_test, cnn_test_acc, mlp_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleList(\n",
      "  (0): Conv_Layer(\n",
      "    3, 128, kernel_size=(5, 5), stride=(1, 1)\n",
      "    (batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (cos_similarity_2): CosineSimilarity()\n",
      "    (cos_similarity_3): CosineSimilarity()\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.0001)\n",
      "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (1): Conv_Layer(\n",
      "    128, 128, kernel_size=(5, 5), stride=(1, 1)\n",
      "    (batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (cos_similarity_2): CosineSimilarity()\n",
      "    (cos_similarity_3): CosineSimilarity()\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.0001)\n",
      "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 34320/34320 [35:08<00:00, 16.27it/s, epoch=15, loss=[0.4803622091983583, 0.2819560589464189]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference from CNNs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1144/1144 [00:45<00:00, 25.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 80.74874344405595%\n",
      "\n",
      "Layerwise Train Accuracy:\n",
      "Layer 1: 73.08648382867133%\n",
      "Layer 2: 80.74874344405595%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New Dataloader: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1144/1144 [00:12<00:00, 95.27it/s]\n",
      "Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11440/11440 [00:18<00:00, 634.34it/s, epoch=10, loss=[0.17161271760852376]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference from End-to-End Network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1144/1144 [00:01<00:00, 946.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 88.84396853146853%\n",
      "Testing on CNNs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1144/1144 [00:44<00:00, 25.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 80.7268902972028%\n",
      "\n",
      "Layerwise Test Accuracy:\n",
      "Layer 1: 73.07145979020979%\n",
      "Layer 2: 80.7268902972028%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New Dataloader: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1144/1144 [00:10<00:00, 105.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on End-to-End Network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1144/1144 [00:01<00:00, 960.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 88.83987106643356%\n",
      "CNN Train Accuracy: 80.7268902972028%\n",
      "MLP Train Accuracy: 88.83987106643356%\n",
      "Testing on CNNs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 407/407 [00:15<00:00, 25.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 77.88875230485556%\n",
      "\n",
      "Layerwise Test Accuracy:\n",
      "Layer 1: 70.24815611555009%\n",
      "Layer 2: 77.88875230485556%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New Dataloader: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 407/407 [00:03<00:00, 105.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on End-to-End Network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 543/543 [00:00<00:00, 1290.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 85.20666871542717%\n",
      "CNN Test Accuracy: 77.88875230485556%\n",
      "MLP Test Accuracy: 85.20666871542717%\n",
      "ModuleList(\n",
      "  (0): Conv_Layer(\n",
      "    3, 128, kernel_size=(5, 5), stride=(1, 1)\n",
      "    (batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (cos_similarity_2): CosineSimilarity()\n",
      "    (cos_similarity_3): CosineSimilarity()\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.0001)\n",
      "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (1): Conv_Layer(\n",
      "    128, 128, kernel_size=(5, 5), stride=(1, 1)\n",
      "    (batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (cos_similarity_2): CosineSimilarity()\n",
      "    (cos_similarity_3): CosineSimilarity()\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.0001)\n",
      "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 34320/34320 [21:49<00:00, 26.21it/s, epoch=15, loss=[0.5010102692652831, 0.2822942599341587]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference from CNNs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1144/1144 [00:44<00:00, 25.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 80.60942963286713%\n",
      "\n",
      "Layerwise Train Accuracy:\n",
      "Layer 1: 72.90756118881119%\n",
      "Layer 2: 80.60942963286713%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New Dataloader: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1144/1144 [00:10<00:00, 105.56it/s]\n",
      "Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11440/11440 [00:17<00:00, 636.51it/s, epoch=10, loss=[0.16945073335477107]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference from End-to-End Network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1144/1144 [00:01<00:00, 906.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 88.28261582167832%\n",
      "Testing on CNNs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1144/1144 [00:44<00:00, 25.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 80.6135270979021%\n",
      "\n",
      "Layerwise Test Accuracy:\n",
      "Layer 1: 72.89936625874127%\n",
      "Layer 2: 80.6135270979021%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New Dataloader: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1144/1144 [00:10<00:00, 105.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on End-to-End Network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1144/1144 [00:01<00:00, 951.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 88.25666520979021%\n",
      "CNN Train Accuracy: 80.6135270979021%\n",
      "MLP Train Accuracy: 88.25666520979021%\n",
      "Testing on CNNs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 407/407 [00:15<00:00, 25.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 77.97326367547633%\n",
      "\n",
      "Layerwise Test Accuracy:\n",
      "Layer 1: 70.37492317148126%\n",
      "Layer 2: 77.97326367547633%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New Dataloader: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 407/407 [00:03<00:00, 105.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on End-to-End Network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 543/543 [00:00<00:00, 1224.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 84.92240319606637%\n",
      "CNN Test Accuracy: 77.97326367547633%\n",
      "MLP Test Accuracy: 84.92240319606637%\n",
      "ModuleList(\n",
      "  (0): Conv_Layer(\n",
      "    3, 128, kernel_size=(5, 5), stride=(1, 1)\n",
      "    (batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (cos_similarity_2): CosineSimilarity()\n",
      "    (cos_similarity_3): CosineSimilarity()\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.0001)\n",
      "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (1): Conv_Layer(\n",
      "    128, 128, kernel_size=(5, 5), stride=(1, 1)\n",
      "    (batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (cos_similarity_2): CosineSimilarity()\n",
      "    (cos_similarity_3): CosineSimilarity()\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.0001)\n",
      "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 34320/34320 [21:50<00:00, 26.19it/s, epoch=15, loss=[0.4822305446633923, 0.2794354719507109]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference from CNNs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1144/1144 [00:45<00:00, 25.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 81.37292395104895%\n",
      "\n",
      "Layerwise Train Accuracy:\n",
      "Layer 1: 73.17662805944056%\n",
      "Layer 2: 81.37292395104895%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New Dataloader: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1144/1144 [00:10<00:00, 105.11it/s]\n",
      "Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11440/11440 [00:18<00:00, 610.51it/s, epoch=10, loss=[0.17108375271065887]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference from End-to-End Network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1144/1144 [00:01<00:00, 976.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 88.83304195804196%\n",
      "Testing on CNNs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1144/1144 [00:45<00:00, 25.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 81.31009615384616%\n",
      "\n",
      "Layerwise Test Accuracy:\n",
      "Layer 1: 73.22443181818183%\n",
      "Layer 2: 81.31009615384616%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New Dataloader: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1144/1144 [00:10<00:00, 105.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on End-to-End Network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1144/1144 [00:01<00:00, 986.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 88.79343312937063%\n",
      "CNN Train Accuracy: 81.31009615384616%\n",
      "MLP Train Accuracy: 88.79343312937063%\n",
      "Testing on CNNs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 407/407 [00:16<00:00, 25.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 78.86831591886909%\n",
      "\n",
      "Layerwise Test Accuracy:\n",
      "Layer 1: 70.18285187461586%\n",
      "Layer 2: 78.86831591886909%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New Dataloader: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 407/407 [00:03<00:00, 105.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on End-to-End Network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 543/543 [00:00<00:00, 1239.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 84.87246465888137%\n",
      "CNN Test Accuracy: 78.86831591886909%\n",
      "MLP Test Accuracy: 84.87246465888137%\n",
      "ModuleList(\n",
      "  (0): Conv_Layer(\n",
      "    3, 128, kernel_size=(5, 5), stride=(1, 1)\n",
      "    (batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (cos_similarity_2): CosineSimilarity()\n",
      "    (cos_similarity_3): CosineSimilarity()\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.0001)\n",
      "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (1): Conv_Layer(\n",
      "    128, 128, kernel_size=(5, 5), stride=(1, 1)\n",
      "    (batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (cos_similarity_2): CosineSimilarity()\n",
      "    (cos_similarity_3): CosineSimilarity()\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.0001)\n",
      "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 34320/34320 [20:50<00:00, 27.44it/s, epoch=15, loss=[0.4765767237575322, 0.2730429618248158]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference from CNNs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1144/1144 [00:42<00:00, 26.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 80.17373251748252%\n",
      "\n",
      "Layerwise Train Accuracy:\n",
      "Layer 1: 72.74093094405595%\n",
      "Layer 2: 80.17373251748252%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New Dataloader: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1144/1144 [00:10<00:00, 107.24it/s]\n",
      "Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11440/11440 [00:16<00:00, 699.92it/s, epoch=10, loss=[0.16931104116219953]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference from End-to-End Network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1144/1144 [00:01<00:00, 1054.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 88.15422858391608%\n",
      "Testing on CNNs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1144/1144 [00:42<00:00, 26.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 80.2229020979021%\n",
      "\n",
      "Layerwise Test Accuracy:\n",
      "Layer 1: 72.69176136363636%\n",
      "Layer 2: 80.2229020979021%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New Dataloader: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1144/1144 [00:10<00:00, 106.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on End-to-End Network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1144/1144 [00:01<00:00, 1036.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 88.09003496503497%\n",
      "CNN Train Accuracy: 80.2229020979021%\n",
      "MLP Train Accuracy: 88.09003496503497%\n",
      "Testing on CNNs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 407/407 [00:15<00:00, 26.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 77.370159803319%\n",
      "\n",
      "Layerwise Test Accuracy:\n",
      "Layer 1: 69.79870928088506%\n",
      "Layer 2: 77.370159803319%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New Dataloader: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 407/407 [00:03<00:00, 108.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on End-to-End Network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 543/543 [00:00<00:00, 1287.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 85.04532882606023%\n",
      "CNN Test Accuracy: 77.370159803319%\n",
      "MLP Test Accuracy: 85.04532882606023%\n",
      "ModuleList(\n",
      "  (0): Conv_Layer(\n",
      "    3, 128, kernel_size=(5, 5), stride=(1, 1)\n",
      "    (batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (cos_similarity_2): CosineSimilarity()\n",
      "    (cos_similarity_3): CosineSimilarity()\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.0001)\n",
      "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (1): Conv_Layer(\n",
      "    128, 128, kernel_size=(5, 5), stride=(1, 1)\n",
      "    (batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (cos_similarity_2): CosineSimilarity()\n",
      "    (cos_similarity_3): CosineSimilarity()\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.0001)\n",
      "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 34320/34320 [20:56<00:00, 27.31it/s, epoch=15, loss=[0.49357289492667156, 0.2811670110697717]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference from CNNs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1144/1144 [00:45<00:00, 25.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 81.00005463286713%\n",
      "\n",
      "Layerwise Train Accuracy:\n",
      "Layer 1: 73.5891062062937%\n",
      "Layer 2: 81.00005463286713%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New Dataloader: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1144/1144 [00:10<00:00, 104.90it/s]\n",
      "Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11440/11440 [00:16<00:00, 673.32it/s, epoch=10, loss=[0.16826828700731267]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference from End-to-End Network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1144/1144 [00:01<00:00, 1028.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 88.5776333041958%\n",
      "Testing on CNNs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1144/1144 [00:44<00:00, 25.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 81.03283435314685%\n",
      "\n",
      "Layerwise Test Accuracy:\n",
      "Layer 1: 73.62325174825175%\n",
      "Layer 2: 81.03283435314685%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New Dataloader: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1144/1144 [00:10<00:00, 105.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on End-to-End Network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1144/1144 [00:01<00:00, 1007.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 88.4396853146853%\n",
      "CNN Train Accuracy: 81.03283435314685%\n",
      "MLP Train Accuracy: 88.4396853146853%\n",
      "Testing on CNNs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 407/407 [00:15<00:00, 25.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 78.31515058389674%\n",
      "\n",
      "Layerwise Test Accuracy:\n",
      "Layer 1: 70.59772587584511%\n",
      "Layer 2: 78.31515058389674%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New Dataloader: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 407/407 [00:03<00:00, 105.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on End-to-End Network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 543/543 [00:00<00:00, 1297.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 84.58051628764598%\n",
      "CNN Test Accuracy: 78.31515058389674%\n",
      "MLP Test Accuracy: 84.58051628764598%\n",
      "(5, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_runs = 5\n",
    "noise_percentage_list = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "layer_wise_test = []\n",
    "\n",
    "for _ in range(num_runs):\n",
    "# for noise_percentage in noise_percentage_list:\n",
    "        \n",
    "    in_channels = [3]\n",
    "    num_classes = 10\n",
    "    kernel_size_list = [(5, 5), (5, 5)]\n",
    "    # kernel_size_list = [(5, 5)]\n",
    "    batch_norm_list = [True, True]\n",
    "    # batch_norm_list = [True]\n",
    "    pooling_size_list = [(2,2), (2,2)]\n",
    "    # pooling_size_list = [(2,2)]\n",
    "    pooling_type = ['max', 'max']\n",
    "    # pooling_type = ['max']\n",
    "    h_list, w_list, new_kernel_size_list = gen_size_lists(64, 64, kernel_size_list, pooling_size_list)\n",
    "    symm_vector_dim_list = [100, 100]\n",
    "    # symm_vector_dim_list = [100]\n",
    "    kernels = [128, 128]\n",
    "    # kernels = [32]\n",
    "    tmp = np.random.randint(low = 2, high = 10, size = kernels[0]).tolist()\n",
    "    n_symm_vectors = [tmp for _ in kernels]\n",
    "    group_layers = [0, 0]\n",
    "    #group_layers = [0]\n",
    "    kernels = in_channels + kernels\n",
    "    \n",
    "    in_out_kernel_list = []\n",
    "    for i in range(len(new_kernel_size_list)):\n",
    "        if i % 2 == 0:\n",
    "            in_out_kernel_list.append((kernels[i // 2], kernels[(i // 2) + 1]))\n",
    "    \n",
    "    mlp_list = [h_list[-1] * w_list[-1] * kernels[-1], 10]\n",
    "    dropout_list = [0, 0]\n",
    "    \n",
    "    bias_flag = True\n",
    "    \n",
    "    epoch_list = [15, 10]\n",
    "    learning_rate_list = [0.1, 0.1]\n",
    "    \n",
    "    net = Full_Net(in_out_kernel_list,\n",
    "                   mlp_list,\n",
    "                   dropout_list,\n",
    "                   kernel_size_list,\n",
    "                   batch_norm_list, \n",
    "                   pooling_size_list,\n",
    "                   pooling_type, \n",
    "                   epoch_list, \n",
    "                   bias_flag, \n",
    "                   device, \n",
    "                   learning_rate_list,\n",
    "                   h_list, \n",
    "                   w_list, \n",
    "                   symm_vector_dim_list, \n",
    "                   n_symm_vectors, \n",
    "                   group_layers).to(device)\n",
    "\n",
    "    # trainloader = noisy_dataloader(train_loader, noise_percentage, device=device)\n",
    "    net.train(train_loader)\n",
    "\n",
    "    layerwise_acc_test, cnn_train_accuracy, mlp_train_accuracy = net.test(train_loader, flag = True)\n",
    "    print(f\"CNN Train Accuracy: {cnn_train_accuracy * 100}%\")\n",
    "    print(f\"MLP Train Accuracy: {mlp_train_accuracy * 100}%\")\n",
    "    train_acc.append([cnn_train_accuracy, mlp_train_accuracy])\n",
    "\n",
    "    # testloader = noisy_dataloader(test_loader, noise_percentage, device=device)\n",
    "    layerwise_acc_test, cnn_test_accuracy, mlp_test_accuracy = net.test(test_loader, flag = True)\n",
    "    \n",
    "    print(f\"CNN Test Accuracy: {cnn_test_accuracy * 100}%\")\n",
    "    print(f\"MLP Test Accuracy: {mlp_test_accuracy * 100}%\")\n",
    "    test_acc.append([cnn_test_accuracy, mlp_test_accuracy])\n",
    "    layer_wise_test.append(layerwise_acc_test)\n",
    "print(np.shape(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"./data/svhn10_train_acc_cnn_12_cos_bf.npy\", np.array(train_acc))\n",
    "np.save(\"./data/svhn10_test_acc_cnn_12_cos_bf.npy\", np.array(test_acc))\n",
    "np.save(\"./data/svhn10_layerwise_test_acc_cnn_12_cos_bf.npy\", np.array(layer_wise_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[30, 13]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on CNNs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 407/407 [00:15<00:00, 25.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 78.31515058389674%\n",
      "\n",
      "Layerwise Test Accuracy:\n",
      "Layer 1: 70.59772587584511%\n",
      "Layer 2: 78.31515058389674%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New Dataloader: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 407/407 [00:03<00:00, 105.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on End-to-End Network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 543/543 [00:00<00:00, 1244.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 84.58051628764598%\n",
      "CNN Test Accuracy: 78.31515058389674%\n",
      "MLP Test Accuracy: 84.58051628764598%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_, cnn_test_accuracy, mlp_test_accuracy = net.test(test_loader, flag = True)\n",
    "print(f\"CNN Test Accuracy: {cnn_test_accuracy * 100}%\")\n",
    "print(f\"MLP Test Accuracy: {mlp_test_accuracy * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
