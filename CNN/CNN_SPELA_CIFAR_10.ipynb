{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './../../../Models')\n",
    "from sphere_points import generate_points\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import normalize, one_hot\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "import torchvision\n",
    "from torchvision.transforms import *\n",
    "# from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "# load the dataset\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), \n",
    "    transforms.Resize((64, 64))]) # this normalizes to [0,1]\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=64,\n",
    "                                          shuffle=True, \n",
    "                                           collate_fn = lambda x: tuple(x_.to(device) for x_ in default_collate(x)))\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=64,\n",
    "                                         shuffle=False, \n",
    "                                          collate_fn = lambda x: tuple(x_.to(device) for x_ in default_collate(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_loss = nn.CrossEntropyLoss()\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_head_train(inp_embedding, classifier_weights, labels):\n",
    "    inp_embedding = normalize(inp_embedding, p=2, dim=-1)\n",
    "    classifier_output = torch.mm(inp_embedding, classifier_weights)\n",
    "    classifier_output = classifier_output * one_hot(labels, num_classes = num_classes).type(torch.float32)\n",
    "    theta = 1\n",
    "    loss = torch.mean(torch.log(2 - (theta * torch.sum(classifier_output,1))))\n",
    "    # classifier_output = torch.softmax(classifier_output, dim=-1)\n",
    "    # loss = model_loss(classifier_output, one_hot(labels, num_classes = num_classes).type(torch.float32))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_head_test(inp_embedding, classifier_weights, labels):\n",
    "    inp_embedding = normalize(inp_embedding, p=2, dim=-1)\n",
    "    classifier_output = torch.mm(inp_embedding, classifier_weights)\n",
    "    # classifier_output = 1 - (torch.acos(classifier_output)/np.pi)\n",
    "    # classifier_output = torch.softmax(classifier_output, dim=-1)\n",
    "    # loss = model_loss(classifier_output, one_hot(labels, num_classes = num_classes).type(torch.float32))\n",
    "    return torch.argmax(classifier_output, dim=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "# theta = 1\n",
    "# cos_similarity = nn.CosineSimilarity(dim = 1, eps = 1e-6)\n",
    "mse_loss = nn.MSELoss()\n",
    "initial = None\n",
    "\n",
    "# def loss_layer(y_pos, y_neg):\n",
    "#     return torch.mean(torch.log(2 - (theta * cos_similarity(y_pos, y_neg))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (num_data, num_features) => no dimension for batch size please\n",
    "class Layer(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, dropout_prob, bias, device, lr):\n",
    "        super().__init__(in_features, out_features, bias, device)\n",
    "        self.out_features = out_features\n",
    "        self.bias_flag = bias\n",
    "        self.lr = lr\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.dropout = nn.Dropout(p = self.dropout_prob)\n",
    "        self.num_classes = 10\n",
    "        self.dimension = out_features\n",
    "        self.leaky_relu = nn.PReLU(init = 0.001)\n",
    "        self.opt = Adam(self.parameters(), lr = self.lr)\n",
    "        global initial\n",
    "        nn.init.kaiming_normal_(self.weight, mode='fan_in')\n",
    "        # fc1_limit = np.sqrt(6.0 / in_features)\n",
    "        # torch.nn.init.uniform_(self.weight, a=-fc1_limit, b=fc1_limit)\n",
    "        self.directions = generate_points(self.num_classes, self.dimension, steps = 10000, initial_points = initial)\n",
    "        initial = np.array(self.directions)\n",
    "        self.directions = [torch.tensor(t, dtype = torch.float32).to(device) for t in self.directions]\n",
    "        self.direction_weights = torch.zeros((len(self.directions[0]), len(self.directions)), device=device)\n",
    "        for i in range(len(self.directions)):\n",
    "            self.direction_weights[:, i] = normalize(self.directions[i], p = 2, dim=-1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_direction = normalize(x, p = 2, dim = 1)\n",
    "        if self.bias_flag:\n",
    "            return self.leaky_relu(self.dropout(torch.mm(x_direction, self.weight.T) + self.bias.unsqueeze(0)))\n",
    "        else:\n",
    "            return self.leaky_relu(self.dropout(torch.mm(x_direction, self.weight.T)))\n",
    "\n",
    "    def train(self, x, labels):\n",
    "        # opt = Adam(self.parameters(), lr = self.lr)\n",
    "        y = self.forward(x) # shape: (num_data, out_features)\n",
    "        y = normalize(y, p = 2, dim = 1)\n",
    "        '''\n",
    "        directions = torch.zeros_like(y)\n",
    "        for i in range(y.shape[0]):\n",
    "            directions[i, :] = self.directions[label[i]].reshape(1, -1)\n",
    "        loss = loss_layer(y, directions)\n",
    "        '''\n",
    "        loss = classifier_head_train(y, self.direction_weights, labels)\n",
    "        self.opt.zero_grad()\n",
    "        loss.backward(retain_graph = True)\n",
    "        self.opt.step()\n",
    "\n",
    "        return loss.item(), y\n",
    "    \n",
    "    def test(self, x, labels):\n",
    "        with torch.no_grad():\n",
    "            y = self.forward(x)\n",
    "            # max_idx_list \n",
    "            '''\n",
    "            for dat in range(y.shape[0]):\n",
    "                max = -np.inf\n",
    "                max_idx = 0\n",
    "                for i in range(self.num_classes):\n",
    "                    cos_sim = cos_similarity(y[dat, :].unsqueeze(0), self.directions[i].reshape(1, -1))\n",
    "                    if cos_sim > max:\n",
    "                        max = cos_sim\n",
    "                        max_idx = i\n",
    "                max_idx_list.append(max_idx)\n",
    "            '''\n",
    "            max_idx_list = []\n",
    "            max_idx_list = classifier_head_test(y, self.direction_weights, labels)\n",
    "        return torch.tensor(max_idx_list), y\n",
    "\n",
    "class Layer_Net(nn.Module):\n",
    "    def __init__(self, dims_list, dropout_list, bias, epochs, lr, device):\n",
    "        super(Layer_Net, self).__init__()\n",
    "        self.dims_list = dims_list\n",
    "        self.dropout_list = dropout_list\n",
    "        self.bias = bias\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.device = device\n",
    "        self.layers = []\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        global initial\n",
    "        for d in range(len(self.dims_list) - 1):\n",
    "            self.layers += [Layer(self.dims_list[d], self.dims_list[d + 1], self.dropout_list[d], \n",
    "                                  self.bias, self.device, self.lr).to(self.device)]\n",
    "        \n",
    "    def train(self, data_loader):\n",
    "        layer_loss_list = []\n",
    "        for i in range(len(self.layers)):\n",
    "            layer_loss_list.append([])\n",
    "        pbar = tqdm(total = self.epochs * len(data_loader) * len(self.layers), desc = f\"Training\", position = 0, leave = True)\n",
    "        for epoch in range(self.epochs):\n",
    "            loss_agg = [0] * len(self.layers)\n",
    "            for dat in data_loader:\n",
    "                x = dat[0]\n",
    "                label = dat[1]\n",
    "                for i in range(len(self.layers)):\n",
    "                    # with torch.no_grad():\n",
    "                    #     y = self.layers[i].forward(x)\n",
    "                    loss, y = self.layers[i].train(x, label)\n",
    "                    x = y.detach()\n",
    "                    loss_agg[i] += loss / len(data_loader)\n",
    "                    # self.layers[i].zero_grad(set_to_none=True)\n",
    "                    pbar.update(1)\n",
    "            pbar.set_postfix(epoch = epoch + 1, loss = loss_agg)\n",
    "            for i in range(len(self.layers)):\n",
    "                layer_loss_list[i].append(loss_agg[i])\n",
    "        pbar.close()\n",
    "        return layer_loss_list\n",
    "\n",
    "        \n",
    "    def test(self, data_loader):\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            cm_preds = []\n",
    "            cm_labels = []\n",
    "            for dat in tqdm(data_loader, desc = \"Testing\"):\n",
    "                x = dat[0]\n",
    "                label = dat[1]\n",
    "                preds = []\n",
    "                for i in range(len(self.layers)):\n",
    "                    pred, x = self.layers[i].test(x, label)\n",
    "                    preds.append(pred)\n",
    "                correct += (preds[-1] == label.cpu()).sum().item()\n",
    "                cm_preds += preds[-1].cpu().numpy().tolist()\n",
    "                cm_labels += label.cpu().numpy().tolist()\n",
    "                total += label.shape[0]\n",
    "        return correct / total, [cm_preds, cm_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_kernel_classes = None\n",
    "\n",
    "# Takes input, size of kernel and gives output dimension of cuboid\n",
    "def gen_size_lists(h_init_image, w_init_image, kernel_size_list, pooling_kernels, padding_list = None, stride_list = None):\n",
    "    h_list = [h_init_image]\n",
    "    w_list = [w_init_image]\n",
    "\n",
    "    new_kernel_size_list = []\n",
    "    k = len(kernel_size_list) + len(pooling_kernels)\n",
    "    for i in range(k):\n",
    "        if i % 2 == 0:\n",
    "            new_kernel_size_list.append(kernel_size_list[i // 2])\n",
    "        else:\n",
    "            new_kernel_size_list.append(pooling_kernels[i // 2])\n",
    "    \n",
    "    if stride_list is None:\n",
    "        stride_list = [1] * len(new_kernel_size_list)\n",
    "    if padding_list is None:\n",
    "        padding_list = [0] * len(new_kernel_size_list)\n",
    "    \n",
    "    for i in range(k):\n",
    "        if i % 2 == 1:\n",
    "            if new_kernel_size_list[i] is not None:\n",
    "                stride_list[i] = new_kernel_size_list[i][0]\n",
    "\n",
    "    for i in range(len(new_kernel_size_list)):\n",
    "        if new_kernel_size_list[i] is None:\n",
    "            h_list.append(int(h_list[i]))\n",
    "            w_list.append(int(w_list[i]))\n",
    "        else:\n",
    "            h_list.append(int((h_list[i] + 2 * padding_list[i] - new_kernel_size_list[i][0]) / stride_list[i] + 1))\n",
    "            w_list.append(int((w_list[i] + 2 * padding_list[i] - new_kernel_size_list[i][1]) / stride_list[i] + 1))\n",
    "    \n",
    "    h_list = h_list[1 : ]\n",
    "    w_list = w_list[1 : ]\n",
    "\n",
    "    final_h_list, final_w_list = [], []\n",
    "\n",
    "    for i in range(len(h_list) // 2):\n",
    "        if new_kernel_size_list[2 * i + 1] is not None:\n",
    "            final_h_list.append(h_list[2 * i + 1])\n",
    "            final_w_list.append(w_list[2 * i + 1])\n",
    "        else:\n",
    "            final_h_list.append(h_list[2 * i])\n",
    "            final_w_list.append(w_list[2 * i])\n",
    "            \n",
    "    return final_h_list, final_w_list, new_kernel_size_list\n",
    "\n",
    "class Conv_Layer(nn.Conv2d, nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels, \n",
    "                 out_channels, \n",
    "                 kernel_size,\n",
    "                 batch_norm_flag,  \n",
    "                 pooling_size, \n",
    "                 pooling_type, \n",
    "                 bias, \n",
    "                 device, \n",
    "                 lr, \n",
    "                 h_out, \n",
    "                 w_out, \n",
    "                 symm_vector_dim, \n",
    "                 n_symm_vectors):\n",
    "        super(Conv_Layer, self).__init__(in_channels, out_channels, kernel_size)\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.batch_norm_flag = batch_norm_flag\n",
    "        if self.batch_norm_flag:\n",
    "            self.batch_norm = nn.BatchNorm2d(self.out_channels)\n",
    "        else:\n",
    "            self.batch_norm = nn.Identity()\n",
    "        \n",
    "        self.pooling_size = pooling_size\n",
    "        self.pooling_type = pooling_type\n",
    "        self.bias_flag = bias\n",
    "        self.device = device\n",
    "        self.lr = lr\n",
    "        self.h_out = h_out\n",
    "        self.w_out = w_out\n",
    "        self.D = self.h_out * self.w_out\n",
    "        self.symm_vector_dim = symm_vector_dim\n",
    "        self.n_symm_vectors = n_symm_vectors\n",
    "        # Initialize the projection matrix such that each kernel has a trainable projection matrix \n",
    "        # size of input is (batch_size x num_kernels x (h_out * w_out) \n",
    "        # output is (batch_size x num_kernels x symm_vector_dim)\n",
    "        self.mlp_weights = nn.Parameter(torch.normal(0, 1, size = (self.out_channels, self.D, self.symm_vector_dim)).to(self.device), requires_grad = True)\n",
    "\n",
    "        self.cos_similarity_2 = nn.CosineSimilarity(dim = 2, eps = 1e-6)\n",
    "        self.cos_similarity_3 = nn.CosineSimilarity(dim = 3, eps = 1e-6)\n",
    "\n",
    "        global num_classes\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        global init_kernel_classes\n",
    "        if init_kernel_classes is None:\n",
    "            # Assign the classes to each kernel\n",
    "            self.kernel_classes = []\n",
    "            for i in range(self.out_channels):\n",
    "                group_number = list(range(self.n_symm_vectors[i]))\n",
    "                flag = True\n",
    "                while flag:\n",
    "                    group_list = np.random.choice(group_number, self.num_classes, replace = True).tolist()\n",
    "                    if len(set(group_list)) == self.n_symm_vectors[i]:\n",
    "                        flag = False\n",
    "                self.kernel_classes.append(group_list)\n",
    "            init_kernel_classes = self.kernel_classes\n",
    "        else:\n",
    "            self.kernel_classes = init_kernel_classes\n",
    "\n",
    "        self.leaky_relu = nn.LeakyReLU(negative_slope = 0.0001)\n",
    "        \n",
    "        self.pool = None\n",
    "        if self.pooling_size is not None and self.pooling_type is not None:\n",
    "            if self.pooling_type == \"max\":\n",
    "                self.pool = nn.MaxPool2d(self.pooling_size[0])\n",
    "            elif self.pooling_type == \"avg\":\n",
    "                self.pool = nn.AvgPool2d(self.pooling_size[0])\n",
    "\n",
    "        # self.fac = nn.Parameter(torch.tensor(10.0, dtype = torch.float32).to(self.device), requires_grad = True)\n",
    "        \n",
    "        self.opt = Adam(self.parameters(), lr = self.lr)\n",
    "\n",
    "        self.tmp_directions = []\n",
    "        for i in range(self.out_channels):\n",
    "            self.tmp_directions.append(generate_points(self.n_symm_vectors[i], self.symm_vector_dim, steps = 10000, initial_points = None))\n",
    "        self.tmp_directions = [torch.tensor(np.stack(t), dtype = torch.float32).to(self.device) for t in self.tmp_directions]\n",
    "        \n",
    "        # Shape of kernel_directions: (out_channels, n_symm_vectors, symm_vector_dim)\n",
    "        # Broadcast to shape of: (out_channels, num_classes, symm_vector_dim)\n",
    "        \n",
    "        self.kernel_directions = torch.zeros((self.out_channels, self.num_classes, self.symm_vector_dim))\n",
    "        for i in range(self.out_channels):\n",
    "            for j in range(self.num_classes):\n",
    "                self.kernel_directions[i, j, :] = self.tmp_directions[i][self.kernel_classes[i][j], :]\n",
    "        \n",
    "        self.test_kernel_directions = self.kernel_directions.unsqueeze(0).repeat(batch_size, 1, 1, 1).to(self.device)     \n",
    "        self.kernel_directions = torch.swapaxes(self.kernel_directions, 0, 1).to(self.device)\n",
    "        self.all_ones = torch.ones([batch_size, self.out_channels]).to(self.device)\n",
    "\n",
    "    def _conv_forward(self, input, weight, bias):\n",
    "        return F.conv2d(input, weight, bias, self.stride,\n",
    "                        self.padding, self.dilation, self.groups)\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = self._conv_forward(input, self.weight, self.bias)\n",
    "        out = self.batch_norm(out)\n",
    "        out = self.leaky_relu(out)\n",
    "        if self.pooling_size is not None:\n",
    "            out = self.pool(out)\n",
    "        return out\n",
    "    \n",
    "    def train(self, x, labels):\n",
    "        # opt = Adam(self.parameters(), lr = self.lr)\n",
    "        # Shape of y: (num_data, out_channels, h_out, w_out)\n",
    "        y = self.forward(x)\n",
    "        # New Shape of y: (num_data, out_channels, h_out * w_out)\n",
    "        z = torch.flatten(y, start_dim = 2)\n",
    "        proj_y = torch.matmul(z.unsqueeze(2), self.mlp_weights).squeeze(2)\n",
    "        proj_y = normalize(proj_y, p = 2, dim = 2)\n",
    "        cos_sim = self.cos_similarity_2(proj_y, self.kernel_directions[labels, :, :])\n",
    "        loss = mse_loss(self.all_ones[:cos_sim.shape[0], :], cos_sim)\n",
    "        self.opt.zero_grad()\n",
    "        loss.backward(retain_graph = True)\n",
    "        self.opt.step()\n",
    "\n",
    "        return loss.item(), y\n",
    "\n",
    "    def test(self, x):\n",
    "        with torch.no_grad():\n",
    "            y = self.forward(x)\n",
    "            z = torch.flatten(y, start_dim = 2)\n",
    "            proj_y = torch.matmul(z.unsqueeze(2), self.mlp_weights).squeeze(2)\n",
    "            proj_y = normalize(proj_y, p = 2, dim = 2)\n",
    "            proj_y = proj_y.unsqueeze(2).repeat(1, 1, self.num_classes, 1)\n",
    "            cos_sim = self.cos_similarity_3(proj_y, self.test_kernel_directions[ : proj_y.shape[0], :, :, :])\n",
    "            cos_sim = torch.mean(cos_sim, dim = 1).cpu()\n",
    "        return torch.argmax(cos_sim, dim = 1), y\n",
    "\n",
    "# dims list will be of the form [(in_features_1, out_features_1), (in_features_2, out_features_2), ...]\n",
    "class Conv_Net(nn.Module):\n",
    "    def __init__(self, \n",
    "                 conv_list, \n",
    "                 kernel_size_list, \n",
    "                 batch_norm_list, \n",
    "                 pooling_size_list, \n",
    "                 pooling_type, \n",
    "                 epochs, bias, \n",
    "                 device, lr, \n",
    "                 h_list, \n",
    "                 w_list, \n",
    "                 symm_vector_dim_list, \n",
    "                 n_symm_vectors, \n",
    "                 group_layers):\n",
    "        super().__init__()\n",
    "        self.conv_list = conv_list\n",
    "        self.kernel_size_list = kernel_size_list\n",
    "        self.batch_norm_list = batch_norm_list\n",
    "        self.pooling_size_list = pooling_size_list\n",
    "        self.pooling_type = pooling_type\n",
    "        self.epochs = epochs\n",
    "        self.bias_flag = bias\n",
    "        self.lr = lr\n",
    "        self.device = device\n",
    "        self.h_list = h_list\n",
    "        self.w_list = w_list\n",
    "        self.symm_vector_dim_list = symm_vector_dim_list\n",
    "        self.n_symm_vectors = n_symm_vectors\n",
    "        self.group_layers = group_layers\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        global init_kernel_classes\n",
    "        \n",
    "        for i in range(len(self.kernel_size_list)):\n",
    "            if i == 0:\n",
    "                init_kernel_classes = None\n",
    "            elif self.group_layers[i] != self.group_layers[i - 1]:\n",
    "                init_kernel_classes = None\n",
    "\n",
    "            self.layers.append(Conv_Layer(self.conv_list[i][0], \n",
    "                                          self.conv_list[i][1], \n",
    "                                          self.kernel_size_list[i],\n",
    "                                          self.batch_norm_list[i], \n",
    "                                          self.pooling_size_list[i],\n",
    "                                          self.pooling_type[i],\n",
    "                                          self.bias_flag, \n",
    "                                          self.device, \n",
    "                                          self.lr, \n",
    "                                          self.h_list[i], \n",
    "                                          self.w_list[i], \n",
    "                                          self.symm_vector_dim_list[i],\n",
    "                                          self.n_symm_vectors[i]))\n",
    "        \n",
    "        print(self.layers)\n",
    "    \n",
    "    def train(self, data_loader):\n",
    "       \n",
    "        layer_loss_list = []\n",
    "        for i in range(len(self.layers)):\n",
    "            layer_loss_list.append([])\n",
    "        pbar = tqdm(total = self.epochs * len(data_loader) * len(self.layers), desc = f\"Training\", position = 0, leave = True)\n",
    "        for epoch in range(self.epochs):\n",
    "            loss_agg = [0] * len(self.layers)\n",
    "            for dat in data_loader:\n",
    "                x, label = dat\n",
    "                for i in range(len(self.layers)):\n",
    "                    # with torch.no_grad():\n",
    "                    #     y = self.layers[i].forward(x)\n",
    "                    loss, y = self.layers[i].train(x, label)\n",
    "                    # self.layers[i].zero_grad(set_to_none=True)\n",
    "                    x = y.detach()\n",
    "                    loss_agg[i] += loss / len(data_loader)\n",
    "                    pbar.update(1)\n",
    "            pbar.set_postfix(epoch = epoch + 1, loss = loss_agg)\n",
    "            for i in range(len(self.layers)):\n",
    "                layer_loss_list[i].append(loss_agg[i])\n",
    "        pbar.close()\n",
    "        return layer_loss_list\n",
    "    \n",
    "    def forward_pass(self, x):\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(self.layers)):\n",
    "                x = self.layers[i].forward(x)\n",
    "        return x\n",
    "\n",
    "    def test(self, data_loader):\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            cm_preds = []\n",
    "            cm_labels = []\n",
    "            acc_list = [0] * len(self.layers)\n",
    "            for dat in tqdm(data_loader, desc = \"Testing\"):\n",
    "                x, label = dat\n",
    "                preds = []\n",
    "                for i in range(len(self.layers)):\n",
    "                    pred, x = self.layers[i].test(x)\n",
    "                    preds.append(pred)\n",
    "                \n",
    "                for i, lst in enumerate(preds):\n",
    "                    acc_list[i] += (lst == label.cpu()).sum().item()\n",
    "                \n",
    "                correct += (preds[-1] == label.cpu()).sum().item()\n",
    "                cm_preds += preds[-1].cpu().numpy().tolist()\n",
    "                cm_labels += label.cpu().numpy().tolist()\n",
    "                total += label.shape[0]\n",
    "            acc_list = [acc / total for acc in acc_list]\n",
    "        return correct / total, [cm_preds, cm_labels], acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_dataloader(data_loader, conv_layer, device):\n",
    "    new_data, new_label = [], []\n",
    "    for (data, label) in tqdm(data_loader, desc = \"New Dataloader\"):\n",
    "        data = conv_layer.forward_pass(data).cpu()\n",
    "        batch_size = data.shape[0]\n",
    "        new_data.append(torch.flatten(data, start_dim = 1))\n",
    "        new_label.append(label.cpu())\n",
    "    \n",
    "    data = torch.cat(new_data, dim = 0)\n",
    "    label = torch.cat(new_label, dim = 0)\n",
    "    new_data_loader = DataLoader(list(zip(data, label)), \n",
    "                                 batch_size = batch_size, shuffle = True,\n",
    "                                 collate_fn = lambda x: tuple(x_.to(device) for x_ in default_collate(x)))\n",
    "\n",
    "    return new_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Full_Net(nn.Module):\n",
    "    def __init__(self, \n",
    "                 conv_list, \n",
    "                 dims_list, \n",
    "                 dropout_list, \n",
    "                 kernel_size_list, \n",
    "                 batch_norm_list, \n",
    "                 pooling_size_list, \n",
    "                 pooling_type, \n",
    "                 epoch_list, \n",
    "                 bias, \n",
    "                 device, \n",
    "                 learning_rate_list, \n",
    "                 h_list, \n",
    "                 w_list, \n",
    "                 symm_vector_dim_list, \n",
    "                 n_symm_vectors,\n",
    "                 group_layers\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.network = Conv_Net(conv_list, \n",
    "                                kernel_size_list,\n",
    "                                batch_norm_list,  \n",
    "                                pooling_size_list, \n",
    "                                pooling_type, \n",
    "                                epoch_list[0], \n",
    "                                bias, \n",
    "                                device, \n",
    "                                learning_rate_list[0], \n",
    "                                h_list, \n",
    "                                w_list, \n",
    "                                symm_vector_dim_list, \n",
    "                                n_symm_vectors,\n",
    "                                group_layers).to(device)\n",
    "        self.layers = Layer_Net(dims_list, \n",
    "                                dropout_list, \n",
    "                                bias, \n",
    "                                epoch_list[1], \n",
    "                                learning_rate_list[1], \n",
    "                                device).to(device)\n",
    "        \n",
    "        self.train_dataloader = None\n",
    "        self.test_dataloader = None\n",
    "        self.device = device\n",
    "        self.cm_train_preds = []\n",
    "        self.cm_test_preds = []\n",
    "\n",
    "    def train(self, dataloader):\n",
    "        layer_loss_list = []\n",
    "        layer_loss_list = self.network.train(dataloader)\n",
    "        # Testing on CNNs\n",
    "        print(f\"\\nInference from CNNs\")\n",
    "        acc, tmp, layerwise_acc_train = self.network.test(dataloader)\n",
    "        print(f\"Train Accuracy: {acc * 100}%\\n\")\n",
    "        self.cm_train_preds.append(tmp)\n",
    "        print(\"Layerwise Train Accuracy:\")\n",
    "        for i, acc in enumerate(layerwise_acc_train):\n",
    "            print(f\"Layer {i + 1}: {acc * 100}%\")\n",
    "        \n",
    "        # have to create a new dataloader with the output of the network\n",
    "        self.train_dataloader = new_dataloader(dataloader, self.network, self.device)\n",
    "        layer_loss_list += self.layers.train(self.train_dataloader)\n",
    "\n",
    "        print(f\"\\nInference from End-to-End Network\")\n",
    "        acc, tmp = self.layers.test(self.train_dataloader)\n",
    "        self.cm_train_preds.append(tmp)\n",
    "        print(f\"Train Accuracy: {acc * 100}%\")\n",
    "        return layer_loss_list\n",
    "    \n",
    "    def test(self, dataloader, flag = False):\n",
    "        print(\"Testing on CNNs\")\n",
    "        cnn_test_acc, tmp, layerwise_acc_test = self.network.test(dataloader)\n",
    "        print(f\"Test Accuracy: {cnn_test_acc * 100}%\\n\")\n",
    "        self.cm_test_preds.append(tmp)\n",
    "        print(\"Layerwise Test Accuracy:\")\n",
    "        for i, acc in enumerate(layerwise_acc_test):\n",
    "            print(f\"Layer {i + 1}: {acc * 100}%\")\n",
    "            \n",
    "        modified_dataloader = new_dataloader(dataloader, self.network, self.device)\n",
    "        if flag:\n",
    "            self.test_dataloader = modified_dataloader\n",
    "        \n",
    "        print(\"Testing on End-to-End Network\")\n",
    "        mlp_test_acc, tmp = self.layers.test(modified_dataloader)\n",
    "        print(f\"Test Accuracy: {mlp_test_acc * 100}%\")\n",
    "        self.cm_test_preds.append(tmp)\n",
    "        \n",
    "        return layerwise_acc_test, cnn_test_acc, mlp_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleList(\n",
      "  (0): Conv_Layer(\n",
      "    3, 128, kernel_size=(5, 5), stride=(1, 1)\n",
      "    (batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (cos_similarity_2): CosineSimilarity()\n",
      "    (cos_similarity_3): CosineSimilarity()\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.0001)\n",
      "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (1): Conv_Layer(\n",
      "    128, 128, kernel_size=(5, 5), stride=(1, 1)\n",
      "    (batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (cos_similarity_2): CosineSimilarity()\n",
      "    (cos_similarity_3): CosineSimilarity()\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.0001)\n",
      "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 23460/23460 [14:51<00:00, 26.31it/s, epoch=15, loss=[0.6355059135447976, 0.5749208255649527]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference from CNNs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 782/782 [00:30<00:00, 25.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 50.79%\n",
      "\n",
      "Layerwise Train Accuracy:\n",
      "Layer 1: 44.376%\n",
      "Layer 2: 50.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New Dataloader: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 782/782 [00:08<00:00, 96.50it/s]\n",
      "Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 31250/31250 [00:26<00:00, 1178.05it/s, epoch=10, loss=[0.30551424348831174]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference from End-to-End Network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3125/3125 [00:01<00:00, 2747.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 73.35000000000001%\n",
      "Testing on CNNs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 782/782 [00:28<00:00, 27.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 50.983999999999995%\n",
      "\n",
      "Layerwise Test Accuracy:\n",
      "Layer 1: 44.28%\n",
      "Layer 2: 50.983999999999995%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New Dataloader: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 782/782 [00:07<00:00, 109.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on End-to-End Network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3125/3125 [00:01<00:00, 2744.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 73.116%\n",
      "CNN Train Accuracy: 50.983999999999995%\n",
      "MLP Train Accuracy: 73.116%\n",
      "Testing on CNNs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 157/157 [00:05<00:00, 27.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 49.05%\n",
      "\n",
      "Layerwise Test Accuracy:\n",
      "Layer 1: 40.949999999999996%\n",
      "Layer 2: 49.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New Dataloader: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 157/157 [00:01<00:00, 111.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on End-to-End Network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 625/625 [00:00<00:00, 2769.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 65.02%\n",
      "CNN Test Accuracy: 49.05%\n",
      "MLP Test Accuracy: 65.02%\n",
      "ModuleList(\n",
      "  (0): Conv_Layer(\n",
      "    3, 128, kernel_size=(5, 5), stride=(1, 1)\n",
      "    (batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (cos_similarity_2): CosineSimilarity()\n",
      "    (cos_similarity_3): CosineSimilarity()\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.0001)\n",
      "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (1): Conv_Layer(\n",
      "    128, 128, kernel_size=(5, 5), stride=(1, 1)\n",
      "    (batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (cos_similarity_2): CosineSimilarity()\n",
      "    (cos_similarity_3): CosineSimilarity()\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.0001)\n",
      "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 23460/23460 [15:09<00:00, 25.80it/s, epoch=15, loss=[0.6381936688404868, 0.5796711956677235]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference from CNNs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 782/782 [00:28<00:00, 27.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 51.598%\n",
      "\n",
      "Layerwise Train Accuracy:\n",
      "Layer 1: 44.214%\n",
      "Layer 2: 51.598%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New Dataloader: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 782/782 [00:07<00:00, 108.14it/s]\n",
      "Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 31250/31250 [00:26<00:00, 1184.06it/s, epoch=10, loss=[0.30346183584690095]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference from End-to-End Network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3125/3125 [00:01<00:00, 2662.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 73.342%\n",
      "Testing on CNNs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 782/782 [00:29<00:00, 26.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 51.80200000000001%\n",
      "\n",
      "Layerwise Test Accuracy:\n",
      "Layer 1: 44.226%\n",
      "Layer 2: 51.80200000000001%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New Dataloader: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 782/782 [00:07<00:00, 106.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on End-to-End Network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3125/3125 [00:01<00:00, 2633.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 73.202%\n",
      "CNN Train Accuracy: 51.80200000000001%\n",
      "MLP Train Accuracy: 73.202%\n",
      "Testing on CNNs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 157/157 [00:05<00:00, 26.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 49.74%\n",
      "\n",
      "Layerwise Test Accuracy:\n",
      "Layer 1: 40.81%\n",
      "Layer 2: 49.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New Dataloader: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 157/157 [00:01<00:00, 107.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on End-to-End Network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 625/625 [00:00<00:00, 2390.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 65.36%\n",
      "CNN Test Accuracy: 49.74%\n",
      "MLP Test Accuracy: 65.36%\n",
      "ModuleList(\n",
      "  (0): Conv_Layer(\n",
      "    3, 128, kernel_size=(5, 5), stride=(1, 1)\n",
      "    (batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (cos_similarity_2): CosineSimilarity()\n",
      "    (cos_similarity_3): CosineSimilarity()\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.0001)\n",
      "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (1): Conv_Layer(\n",
      "    128, 128, kernel_size=(5, 5), stride=(1, 1)\n",
      "    (batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (cos_similarity_2): CosineSimilarity()\n",
      "    (cos_similarity_3): CosineSimilarity()\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.0001)\n",
      "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 23460/23460 [15:09<00:00, 25.79it/s, epoch=15, loss=[0.638406306276541, 0.5814666638288966]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference from CNNs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 782/782 [00:33<00:00, 23.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 49.748%\n",
      "\n",
      "Layerwise Train Accuracy:\n",
      "Layer 1: 43.84%\n",
      "Layer 2: 49.748%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New Dataloader: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 782/782 [00:07<00:00, 101.29it/s]\n",
      "Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 31250/31250 [00:36<00:00, 850.78it/s, epoch=10, loss=[0.3031764376640314]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference from End-to-End Network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3125/3125 [00:01<00:00, 2055.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 73.19200000000001%\n",
      "Testing on CNNs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 782/782 [00:41<00:00, 18.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 49.732%\n",
      "\n",
      "Layerwise Test Accuracy:\n",
      "Layer 1: 43.86%\n",
      "Layer 2: 49.732%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New Dataloader: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 782/782 [00:10<00:00, 72.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on End-to-End Network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3125/3125 [00:03<00:00, 933.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 73.08200000000001%\n",
      "CNN Train Accuracy: 49.732%\n",
      "MLP Train Accuracy: 73.08200000000001%\n",
      "Testing on CNNs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 157/157 [00:09<00:00, 15.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 47.47%\n",
      "\n",
      "Layerwise Test Accuracy:\n",
      "Layer 1: 40.44%\n",
      "Layer 2: 47.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New Dataloader: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 157/157 [00:02<00:00, 72.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on End-to-End Network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 625/625 [00:00<00:00, 1086.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 64.27000000000001%\n",
      "CNN Test Accuracy: 47.47%\n",
      "MLP Test Accuracy: 64.27000000000001%\n",
      "ModuleList(\n",
      "  (0): Conv_Layer(\n",
      "    3, 128, kernel_size=(5, 5), stride=(1, 1)\n",
      "    (batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (cos_similarity_2): CosineSimilarity()\n",
      "    (cos_similarity_3): CosineSimilarity()\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.0001)\n",
      "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (1): Conv_Layer(\n",
      "    128, 128, kernel_size=(5, 5), stride=(1, 1)\n",
      "    (batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (cos_similarity_2): CosineSimilarity()\n",
      "    (cos_similarity_3): CosineSimilarity()\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.0001)\n",
      "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 23460/23460 [17:56<00:00, 21.79it/s, epoch=15, loss=[0.6233701879715985, 0.5605597971650336]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference from CNNs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 782/782 [00:30<00:00, 25.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 52.03%\n",
      "\n",
      "Layerwise Train Accuracy:\n",
      "Layer 1: 44.214%\n",
      "Layer 2: 52.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New Dataloader: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 782/782 [00:07<00:00, 104.83it/s]\n",
      "Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 31250/31250 [00:28<00:00, 1080.06it/s, epoch=10, loss=[0.309697022395134]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference from End-to-End Network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3125/3125 [00:01<00:00, 2475.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 73.602%\n",
      "Testing on CNNs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 782/782 [00:30<00:00, 25.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 51.99399999999999%\n",
      "\n",
      "Layerwise Test Accuracy:\n",
      "Layer 1: 44.198%\n",
      "Layer 2: 51.99399999999999%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New Dataloader: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 782/782 [00:07<00:00, 105.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on End-to-End Network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3125/3125 [00:01<00:00, 2597.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 73.604%\n",
      "CNN Train Accuracy: 51.99399999999999%\n",
      "MLP Train Accuracy: 73.604%\n",
      "Testing on CNNs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 157/157 [00:05<00:00, 26.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 49.57%\n",
      "\n",
      "Layerwise Test Accuracy:\n",
      "Layer 1: 40.29%\n",
      "Layer 2: 49.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New Dataloader: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 157/157 [00:01<00:00, 105.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on End-to-End Network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 625/625 [00:00<00:00, 2558.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 65.03999999999999%\n",
      "CNN Test Accuracy: 49.57%\n",
      "MLP Test Accuracy: 65.03999999999999%\n",
      "ModuleList(\n",
      "  (0): Conv_Layer(\n",
      "    3, 128, kernel_size=(5, 5), stride=(1, 1)\n",
      "    (batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (cos_similarity_2): CosineSimilarity()\n",
      "    (cos_similarity_3): CosineSimilarity()\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.0001)\n",
      "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (1): Conv_Layer(\n",
      "    128, 128, kernel_size=(5, 5), stride=(1, 1)\n",
      "    (batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (cos_similarity_2): CosineSimilarity()\n",
      "    (cos_similarity_3): CosineSimilarity()\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.0001)\n",
      "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 23460/23460 [14:49<00:00, 26.36it/s, epoch=15, loss=[0.6395803138118269, 0.5828255216026551]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference from CNNs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 782/782 [00:30<00:00, 25.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 51.656%\n",
      "\n",
      "Layerwise Train Accuracy:\n",
      "Layer 1: 44.022%\n",
      "Layer 2: 51.656%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New Dataloader: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 782/782 [00:07<00:00, 106.83it/s]\n",
      "Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 31250/31250 [00:27<00:00, 1124.76it/s, epoch=10, loss=[0.31060223807334897]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference from End-to-End Network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3125/3125 [00:01<00:00, 2553.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 71.756%\n",
      "Testing on CNNs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 782/782 [00:30<00:00, 25.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 51.732%\n",
      "\n",
      "Layerwise Test Accuracy:\n",
      "Layer 1: 43.97%\n",
      "Layer 2: 51.732%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New Dataloader: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 782/782 [00:07<00:00, 106.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on End-to-End Network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3125/3125 [00:01<00:00, 2549.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 71.5%\n",
      "CNN Train Accuracy: 51.732%\n",
      "MLP Train Accuracy: 71.5%\n",
      "Testing on CNNs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 157/157 [00:06<00:00, 25.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 49.730000000000004%\n",
      "\n",
      "Layerwise Test Accuracy:\n",
      "Layer 1: 40.56%\n",
      "Layer 2: 49.730000000000004%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New Dataloader:  56%|███████████████████████████████████████████████████████████████████████████████████▌                                                                 | 88/157 [00:00<00:00, 107.16it/s]"
     ]
    }
   ],
   "source": [
    "num_runs = 5\n",
    "noise_percentage_list = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "layer_wise_test = []\n",
    "\n",
    "for _ in range(num_runs):\n",
    "# for noise_percentage in noise_percentage_list:\n",
    "        \n",
    "    in_channels = [3]\n",
    "    num_classes = 10\n",
    "    kernel_size_list = [(5, 5), (5, 5)]\n",
    "    # kernel_size_list = [(5, 5)]\n",
    "    batch_norm_list = [True, True]\n",
    "    # batch_norm_list = [True]\n",
    "    pooling_size_list = [(2,2), (2,2)]\n",
    "    # pooling_size_list = [(2,2)]\n",
    "    pooling_type = ['max', 'max']\n",
    "    # pooling_type = ['max']\n",
    "    h_list, w_list, new_kernel_size_list = gen_size_lists(64, 64, kernel_size_list, pooling_size_list)\n",
    "    symm_vector_dim_list = [100, 100]\n",
    "    # symm_vector_dim_list = [100]\n",
    "    kernels = [128, 128]\n",
    "    # kernels = [32]\n",
    "    tmp = np.random.randint(low = 2, high = 10, size = kernels[0]).tolist()\n",
    "    n_symm_vectors = [tmp for _ in kernels]\n",
    "    group_layers = [0, 0]\n",
    "    #group_layers = [0]\n",
    "    kernels = in_channels + kernels\n",
    "    \n",
    "    in_out_kernel_list = []\n",
    "    for i in range(len(new_kernel_size_list)):\n",
    "        if i % 2 == 0:\n",
    "            in_out_kernel_list.append((kernels[i // 2], kernels[(i // 2) + 1]))\n",
    "    \n",
    "    mlp_list = [h_list[-1] * w_list[-1] * kernels[-1], 10]\n",
    "    dropout_list = [0, 0]\n",
    "    \n",
    "    bias_flag = True\n",
    "    \n",
    "    epoch_list = [15, 10]\n",
    "    learning_rate_list = [0.1, 0.1]\n",
    "    \n",
    "    net = Full_Net(in_out_kernel_list,\n",
    "                   mlp_list,\n",
    "                   dropout_list,\n",
    "                   kernel_size_list,\n",
    "                   batch_norm_list, \n",
    "                   pooling_size_list,\n",
    "                   pooling_type, \n",
    "                   epoch_list, \n",
    "                   bias_flag, \n",
    "                   device, \n",
    "                   learning_rate_list,\n",
    "                   h_list, \n",
    "                   w_list, \n",
    "                   symm_vector_dim_list, \n",
    "                   n_symm_vectors, \n",
    "                   group_layers).to(device)\n",
    "\n",
    "    # trainloader = noisy_dataloader(train_loader, noise_percentage, device=device)\n",
    "    net.train(train_loader)\n",
    "\n",
    "    layerwise_acc_test, cnn_train_accuracy, mlp_train_accuracy = net.test(train_loader, flag = True)\n",
    "    print(f\"CNN Train Accuracy: {cnn_train_accuracy * 100}%\")\n",
    "    print(f\"MLP Train Accuracy: {mlp_train_accuracy * 100}%\")\n",
    "    train_acc.append([cnn_train_accuracy, mlp_train_accuracy])\n",
    "\n",
    "    # testloader = noisy_dataloader(test_loader, noise_percentage, device=device)\n",
    "    layerwise_acc_test, cnn_test_accuracy, mlp_test_accuracy = net.test(test_loader, flag = True)\n",
    "    \n",
    "    print(f\"CNN Test Accuracy: {cnn_test_accuracy * 100}%\")\n",
    "    print(f\"MLP Test Accuracy: {mlp_test_accuracy * 100}%\")\n",
    "    test_acc.append([cnn_test_accuracy, mlp_test_accuracy])\n",
    "    layer_wise_test.append(layerwise_acc_test)\n",
    "print(np.shape(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"./data/cifar10_train_acc_cnn_12_cos_bf.npy\", np.array(train_acc))\n",
    "np.save(\"./data/cifar10_test_acc_cnn_12_cos_bf.npy\", np.array(test_acc))\n",
    "np.save(\"./data/cifar10_layerwise_test_acc_cnn_12_cos_bf.npy\", np.array(layer_wise_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cnn_test_accuracy, mlp_test_accuracy = net.test(test_loader, flag = True)\n",
    "print(f\"CNN Test Accuracy: {cnn_test_accuracy * 100}%\")\n",
    "print(f\"MLP Test Accuracy: {mlp_test_accuracy * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
